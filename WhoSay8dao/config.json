{
    "architectures": [
      "WhoSay8dao"
    ],
    "auto_map": {
      "AutoConfig": "LMConfig.LMConfig",
      "AutoModelForCausalLM": "model.model"
    },
    "aux_loss_alpha": 0.01,
    "dim": 128,
    "dropout": 0.0,
    "flash_attn": true,
    "hidden_dim": null,
    "max_seq_len": 512,
    "model_type": "MiniMindLM",
    "multiple_of": 64,
    "n_heads": 8,
    "n_kv_heads": 2,
    "n_layers": 4,
    "n_routed_experts": 4,
    "n_shared_experts": true,
    "norm_eps": 1e-05,
    "norm_topk_prob": true,
    "num_experts_per_tok": 2,
    "rope_theta": 10000.0,
    "safe_serialization": false,
    "scoring_func": "softmax",
    "seq_aux": true,
    "torch_dtype": "float32",
    "transformers_version": "4.44.2",
    "use_moe": false,
    "vocab_size": 6400
  }